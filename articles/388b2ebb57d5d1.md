---
title: "Pytorch2.0ã§Flash Attentionã‚’ä½¿ã£ã¦ã¿ãŸè©±"
emoji: "ğŸ”¥"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["AI","PYTORCH"]
published: true
# publication_name: "fusic"
---

ã“ã‚“ã«ã¡ã¯ã€Fusicã®ãƒãƒ³ã§ã™ã€‚[æ ªå¼ä¼šç¤¾Fusic](https://fusic.co.jp/)ã§ã¯æ©Ÿæ¢°å­¦ç¿’é–¢é€£ã®PoCã‹ã‚‰é–‹ç™ºãƒ»é‹ç”¨ã¾ã§æ§˜ã€…ãªã”ç›¸è«‡ã«å¯¾å¿œã—ã¦ã¾ã™ã€‚ã‚‚ã—å›°ã£ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã—ãŸã‚‰æ°—è»½ã«[ãŠå£°ã‹ã‘ã¦ãã ã•ã„](https://fusic.co.jp/contact/)ã€‚


ä»Šå›ã¯Flash Attentionã‚’ä½¿ã£ã¦ã¿ãŸã“ã¨ã«ã¤ã„ã¦ã€ç°¡å˜ã«èª¬æ˜ã—ã‚ˆã†ã¨æ€ã„ã¾ã™ã€‚[FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/pdf/2205.14135.pdf)ã§ç´¹ä»‹ã•ã‚ŒãŸã“ã®Attentionæ–¹æ³•ã¯ã€æ—©ãã¦æ­£ç¢ºãªAttentionã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å®Ÿç¾ã—ã€ã‚ˆã‚Šé•·ã„Sequenceã§ã®Transformerå­¦ç¿’ã‚’è¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚

ã“ã®è¨˜äº‹ã§ã¯ã€Flash Attentionã®ç†è«–çš„ãªã“ã¨ã‚’è§£æã™ã‚‹ã“ã¨ã‚ˆã‚Šã¯ã€Pytorch2.0ã§ã®å®Ÿè£…ã‚’è¡Œã†éš›ã€æ³¨æ„ã™ã¹ããªéƒ¨åˆ†ã‚’æ•´ç†ã—ã¾ã™ã®ã§ã€è«–æ–‡ã®å†…å®¹ã«ã¤ã„ã¦ã¯[FlashAttention - Tri Dao | Stanford MLSys #67](https://www.youtube.com/watch?v=gMOAud7hZg4)ã‚’å‚ç…§ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚

## Flash Attentionã¨ã¯
Flash Attentionã¯ã€**é•·ã„Sequenceã§ã®Transformerå­¦ç¿’ãŒã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹**ã¨ã„ã†ç›®çš„ã§ææ¡ˆã•ã‚ŒãŸæ‰‹æ³•ã§ã€å¾“æ¥ã®Attentionæ–¹æ³•ã§ã®ä»¥ä¸‹ã®ã‚ˆã†ãªå•é¡Œã‚’è§£æ±ºã—ã‚ˆã†ã—ã—ã¦ã„ã¾ã™ã€‚

- é•·ã„Sequenceã§ã®å­¦ç¿’ãŒé›£ã—ã„
- é•·ã„å‡¦ç†ã®ãŸã‚ã«BatchSizeã‚’æ¸›ã‚‰ã™ã¨å­¦ç¿’æ™‚é–“ãŒé•·ããªã‚‹

æŒ‡æ‘˜ã—ã¦ã„ã‚‹ã®ã¯ã€**Qeury x Key ã®ãƒãƒˆãƒªã‚¯ã‚¹è¨ˆç®—**ã®éƒ¨åˆ†ã§ã€N x N ã‚µã‚¤ã‚ºã®è¨ˆç®—ã®éš›ã€GPUã§ã®ãƒ‡ãƒ¼ã‚¿ã‚„ã‚Šã¨ã‚ŠãŒä¸Šè¨˜ã®å•é¡Œã®åŸå› ã«ãªã£ã¦ã„ã‚‹ã¨èª¬æ˜ã—ã¦ã„ã¾ã™ã€‚ä¸‹ã®å›³ã¯è«–æ–‡ã‹ã‚‰å–ã‚Šå‡ºã—ãŸã‚‚ã®ã§ã™ãŒã€IO Awarenessã‚’è€ƒãˆã€GPUã¨SRAMé–“ã®ã‚„ã‚Šå–ã‚Šã‚’åŠ é€ŸåŒ–ã—ãŸå†…å®¹ã‚’è¡¨ã—ã¦ã„ã¾ã™ã€‚

![](https://drive.google.com/uc?id=1lfX3EJcQVMd13nB4QMVYuByUBSY2sM6F)
*Flash Attention:å‚ç…§è«–æ–‡*

Query x Keyã¨Valueã®è¨ˆç®—ã‚’è¤‡æ•°ã®Blockã«åˆ†ã‘ã€SRAMã«é€ã£ã¦å‡¦ç†ã—ã¦ã„ã‚‹ã®ãŒã‚ã‹ã‚Šã¾ã™ã€‚ã“ã®ã‚ˆã†ãªTilingã‚’é€šã˜ã€æ—¢å­˜ã®æ‰‹æ³•ã‚ˆã‚Šæ—©ã(x3)ã€ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚ˆã(x10~20)Attentionè¨ˆç®—ãŒã§ãã€16Kã®Sequenceã«ã‚‚å¯¾å¿œãŒã§ãã‚‹ã‚ˆã†ã«ãªã£ãŸã¨ä¸»å¼µã—ã¦ã„ã¾ã™ã€‚

ä¸‹ã®å›³ã¯ã€ä¸Šã®æ‰‹æ³•ã‚’ä»–ã®å½¢å¼ã§è¡¨ã—ãŸã‚‚ã®ã§ã™ãŒã€Blockã®å‡¦ç†ãŒã‚ˆã‚Šç†è§£ã—ã‚„ã™ã‹ã£ãŸã®ã§æŒã£ã¦ãã¾ã—ãŸã€‚ã”å‚è€ƒãã ã•ã„ã€‚

![](https://drive.google.com/uc?id=1m29C8ATRC-rnV7DEpo0Chdz-hlaaAVg5)
*Blockã«åˆ†ã‘ã¦Attentionã‚’å‡¦ç†ï¼š[å‚ç…§å‹•ç”»](https://www.youtube.com/watch?v=gMOAud7hZg4)*

## Pytorch2.0ã§ã¯Flash Attentionã‚’æ”¯æ´ã—ã¦ã„ã‚‹ï¼Ÿ
çµè«–ã‹ã‚‰è¨€ã†ã¨ã€**è‡ªå‹•çš„ã«Flash Attentionã‚’ä½¿ã†ã‚ˆã†ãªæ§‹é€ ã‚’ã—ã¦ã„ã‚‹ãŒã€ã©ã‚“ãªå ´åˆã§ã‚‚ä½¿ç”¨ã—ã¦ã„ã‚‹ã‚ã‘ã§ã¯ãªã„**ã§ã™ã€‚Pytorch2.0ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ[(BETA) IMPLEMENTING HIGH-PERFORMANCE TRANSFORMERS WITH SCALED DOT PRODUCT ATTENTION (SDPA)](https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html)ã§ã¯ã€CUDAã‚¤ãƒ³ãƒ—ãƒƒãƒˆã«å¯¾ã—ã¦ã€ä»¥ä¸‹ã®ï¼“ã¤ã®æ‰‹æ³•ã‹ã‚‰æœ€ã‚‚ã‚‰ã—ã„ã‚‚ã®ãŒé¸ã°ã‚Œã€SDPAãŒè¡Œã‚ã‚Œã‚‹ã¨æ›¸ã‹ã‚Œã¦ã„ã¾ã™ã€‚

- Flash Attention
- Memory-Efficient Attention
- A PyTorch implementation defined in C++

ã¾ãŸã€æ–°ãŸãªSDPAã¯ã€Œtorch.nn.MultiheadAttentionã€ã‚„ã€Œtorch.nn.TransformerEncoderLayerã€ã®ã‚‚é©ç”¨ã•ã‚Œã¦ã„ã‚‹ã¨æ›¸ã‹ã‚Œã¦ã„ã¾ã™ã€‚æ—©é€Ÿã€å­¦ç¿’ä¸­ã®ãƒ¢ãƒ‡ãƒ«ã‚’Pytorch2.0ã«å¤‰æ›ã—ã€Flash Attentionã®è‰¯ã•ã‚’æ¥½ã—ã‚‚ã†ã¨ã—ã¦ã„ã¾ã—ãŸã€‚ã—ã‹ã—ã€ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®æ”¹å–„ã¯æ„Ÿã˜ã‚‰ã‚Œãšã€èª¿ã¹ãŸçµæœä»¥ä¸‹ã®ã‚ˆã†ãªåˆ¶ç´„ã«æ°—ã¥ãã¾ã—ãŸã€‚

> Flash Attentionã‚’ä½¿ã†ãŸã‚ã«ã¯ã€
> - ãƒ‡ãƒ¼ã‚¿å½¢å¼ã¨ã—ã¦ã€ffloat16 and bfloat16ã‚’ä½¿ã†ã“ã¨ã€‚
> - Cuda Device PropertyãŒå¯¾è±¡ã§ã‚ã‚‹ã“ã¨ã€‚
> - Maskã‚’ä½¿ã‚ãªã„ã“ã¨ã€‚(Triangular Matrixã¯å¯èƒ½)


ä»–ã«ã‚‚ã€Batch Sizeã‚„Headã®æ•°ãƒ»ã‚µã‚¤ã‚ºã«é–¢ã™ã‚‹åˆ¶ç´„ãŒã‚ã‚Šã¾ã™ãŒã€è©³ç´°ã¯[Accelerated PyTorch 2 Transformers](https://pytorch.org/blog/accelerated-pytorch-2/)ã‚’ã”å‚è€ƒãã ã•ã„ã€‚

å¯¾ç­–ã¨ã—ã¦ã€[Flash Attentionã®ã‚ªãƒ•ã‚£ã‚·ãƒ£ãƒ«Code](https://github.com/HazyResearch/flash-attention)ã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™ãŒã€GPUã®åˆ¶ç´„ã€ŒTuring, Ampere, Ada, or Hopper GPUs (e.g., H100, A100, RTX 3090, T4, RTX 2080)ã€ã‚„Head Dimensionsã®åˆ¶ç´„ãªã©ã¯å­˜åœ¨ã—ã¾ã™ã€‚

## ãƒŸãƒ‹ãƒ†ã‚¹ãƒˆ
ä»¥ä¸‹ã®ï¼“ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ãŸç°¡å˜ãªãƒŸãƒ‹ãƒ†ã‚¹ãƒˆã‚’è¡Œã£ã¦ã¿ã¾ã—ãŸã€‚
- Pytorch2.0 Multi Head Attention
- Pytorch2.0 SDPAã‚’ç”¨ã„ãŸ Custom Multi Head Attention
- Official Flash Attentionã‚’ç”¨ã„ãŸ Multi Head Attention

Inputå½¢ã¯(32, 128, 500)ã§ã‚ã‚Šã€Sequenceã‚µã‚¤ã‚ºãŒ500ã€Train Stepã¯100ã«ã—ã¾ã—ãŸã€‚

### ã‚ªãƒ—ã‚·ãƒ§ãƒ³ç„¡ã—ã®æ“¬æœ¬çš„ãªAttention
Triangular Matrixã‚„ä»–ã®ãƒã‚¹ã‚­ãƒ³ã‚°ã€Dropoutã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä»˜ã‘ãšã«ãƒ†ã‚¹ãƒˆã—ã¾ã—ãŸã€‚çµæœã€Originalãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šï¼“ã¤ã®ãƒ¢ãƒ‡ãƒ«å…¨éƒ¨ã€å­¦ç¿’é€Ÿåº¦ã®æ”¹å–„ãŒã‚ã‚Šã¾ã—ãŸã€‚

```python
----------------------------------------------------------
dropout_p: 0.0, attn_mask_is_none: True
----------------------------------------------------------
Original Multi Head Attention
265132.048
----------------------------------------------------------
Pytorch2.0 Multi Head Attention
102817.892
----------------------------------------------------------
Pytorch2.0 SDPAã‚’ç”¨ã„ãŸ Custom Multi Head Attention
100210.796
----------------------------------------------------------
Official Flash Attentionã‚’ç”¨ã„ãŸ Multi Head Attention
107506.928
----------------------------------------------------------
```

### Dropoutã‚ªãƒ—ã‚·ãƒ§ãƒ³è¿½åŠ 
Dropoutã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ ã—ãŸçµæœã§ã¯ã€Pytorch2.0 Multi Head Attentionã‚’ç”¨ã„ãŸæ‰‹æ³•ãŒã€Original Multi Head Attentionã¨åŒã˜é€Ÿåº¦ã‚’è¡¨ã—ã¾ã—ãŸã€‚[Pytorchã®ã‚³ãƒ¼ãƒ‰](https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#MultiheadAttention)ã‚’è¦‹ãŸé™ã‚Šã€Dropoutã«é–¢ã™ã‚‹åˆ¶ç´„ã¯ç„¡ã•ãã†ã§ã™ãŒã€çµæœçš„ã«ã¯å¾“æ¥ã®æ‰‹æ³•ã‚’ä½¿ã£ãŸå­¦ç¿’ã‚’è¡Œãªã£ãŸæ„Ÿã˜ã§ã™ã€‚

```python
----------------------------------------------------------
dropout_p: 0.0, attn_mask_is_none: False
----------------------------------------------------------
Pytorch2.0 Multi Head Attention
249348.034
----------------------------------------------------------
Pytorch2.0 SDPAã‚’ç”¨ã„ãŸ Custom Multi Head Attention
100304.358
----------------------------------------------------------
Official Flash Attentionã‚’ç”¨ã„ãŸ Multi Head Attention
95603.849
----------------------------------------------------------
```

### Maskã‚ªãƒ—ã‚·ãƒ§ãƒ³è¿½åŠ 
Maskã‚’ä½œã‚Šã€Attentionã‚¿ã‚¹ã‚¯ã§ãƒã‚¹ã‚­ãƒ³ã‚°ã‚’è¡Œã†ã‚ˆã†ã«è¨­å®šã—ã¾ã—ãŸã€‚Triangular Matrixï¼ˆis_casualï¼‰ä»¥å¤–ã®ãƒã‚¹ã‚­ãƒ³ã‚°ã¯æ”¯æ´ã—ã¦ç„¡ã„ãŸã‚ã€Pytorch2.0ã®æ‰‹æ³•ã§ã¯Original Multi Head Attentionã¨åŒæ§˜ãªæ€§èƒ½ã‚’è¦‹ã›ã¾ã—ãŸã€‚

```python
----------------------------------------------------------
dropout_p: 0.1, attn_mask_is_none: True
----------------------------------------------------------
Pytorch2.0 Multi Head Attention
242825.850
----------------------------------------------------------
Pytorch2.0 SDPAã‚’ç”¨ã„ãŸ Custom Multi Head Attention
242832.022
----------------------------------------------------------
Official Flash Attentionã‚’ç”¨ã„ãŸ Multi Head Attention
90324.201
----------------------------------------------------------
```

## Pytorch2.0ã®Flash Attentionã‚’ä½¿ã£ã¦ã¿ãŸæ„Ÿæƒ³
æ„å¤–ã¨åˆ¶ç´„ãŒã‚ã‚Šã€SDPAã‚’ãƒ™ãƒ¼ã‚¹ã«ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã¨æ€ã„ã¾ã—ãŸã€‚ç‰¹ã«Batchå†…ã§ç•°ãªã‚‹é•·ã•ã‚’æŒã¤ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã™ã‚‹ãŸã‚ã«ã¯MaskãŒå¿…è¦ã§ã‚ã‚Šã€å˜ç´”ã«Attentioné–¢æ•°ã‚’å¤‰æ›ã™ã‚‹ã ã‘ã§ã¯å¿œç”¨ã§ããªã„ã®ã§ã¯ãªã„ã‹ã¨æ€ã„ã¾ã—ãŸ
ã¾ãŸã€è‡ªå‹•ã§ã€ŒFlash Attentionãƒ»Memory-Efficient Attentionãƒ»Original Attentionã€ã®ä¸­ã§å®Ÿè¡Œã•ã‚Œã‚‹ã®ã§ã€ã©ã®ã‚ˆã†ãªæ‰‹æ³•ãŒä½¿ã‚ã‚Œã¦ã„ã‚‹ã‹ã‚’ã—ã£ã‹ã‚Šç¢ºèªã—ãªãŒã‚‰è©¦ã™å¿…è¦ãŒã‚ã‚‹ã¨æ€ã„ã¾ã™ã€‚


